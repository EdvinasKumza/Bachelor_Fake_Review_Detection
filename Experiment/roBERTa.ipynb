{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8218ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\" \n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "\n",
    "info = tf.sysconfig.get_build_info()\n",
    "print(\"Built against CUDA:\",  info.get(\"cuda_version\"))\n",
    "print(\"Built against cuDNN:\", info.get(\"cudnn_version\"))\n",
    "print(\"GPUs found: \",         tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "tf.config.experimental.enable_op_determinism()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90876108",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SEED = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(GLOBAL_SEED)\n",
    "tf.random.set_seed(GLOBAL_SEED)\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e07478",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"TF:\",     tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4310f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../Experiment/Datasets/Generated Fake Amazon Reviews Dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11de4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    df_amazon_phase1_data, df_amazon_test_data = train_test_split(\n",
    "        df, test_size=0.2, random_state=GLOBAL_SEED, stratify=df['label']\n",
    "    )\n",
    "\n",
    "    X_amazon_test_raw = df_amazon_test_data['text_']\n",
    "    y_amazon_test_raw = df_amazon_test_data['label']\n",
    "\n",
    "\n",
    "    X_train_amazon_text = df_amazon_phase1_data['text_']\n",
    "    y_train_amazon_raw = df_amazon_phase1_data['label']\n",
    "\n",
    "    texts_train_amazon = X_train_amazon_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c9e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_file_path = '../Experiment/Datasets/Mixed Yelp Dataset.csv'\n",
    "df_yelp = pd.read_csv(yelp_file_path)\n",
    "\n",
    "df_yelp_phase2_data, df_yelp_test_data = train_test_split(\n",
    "    df_yelp, test_size=0.2, random_state=GLOBAL_SEED, stratify=df_yelp['LABEL']\n",
    ")\n",
    "\n",
    "X_yelp_test_raw = df_yelp_test_data['REVIEW_TEXT']\n",
    "y_yelp_test_raw = df_yelp_test_data['LABEL'].replace({-1: 'CG', 1: 'OR'})\n",
    "    \n",
    "X_train_yelp_text = df_yelp_phase2_data['REVIEW_TEXT']\n",
    "y_train_yelp_raw = df_yelp_phase2_data['LABEL'].replace({-1: 'CG', 1: 'OR'}) \n",
    "        \n",
    "\n",
    "texts_train_yelp = X_train_yelp_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e34e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification\n",
    "\n",
    "MODEL_NAME = 'FacebookAI/roberta-base' \n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "num_labels = 2 \n",
    "model = TFRobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'CG': 0, 'OR': 1} \n",
    "\n",
    "y_train_amazon_int = y_train_amazon_raw.map(label_map).values \n",
    "y_amazon_test_int = y_amazon_test_raw.map(label_map).values\n",
    "\n",
    "\n",
    "y_train_yelp_int = y_train_yelp_raw.map(label_map).values\n",
    "y_yelp_test_int = y_yelp_test_raw.map(label_map).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "train_encodings_amazon = tokenizer(texts_train_amazon, truncation=True, padding=True, max_length=max_length, return_tensors=\"tf\")\n",
    "# train_encodings_yelp = tokenizer(texts_train_yelp, truncation=True, padding=True, max_length=max_length, return_tensors=\"tf\") # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametrų tinklelio paieška\n",
    "\n",
    "# learning_rates = [1e-5, 3e-5, 5e-5]\n",
    "# epochs_options = [1, 2, 3]\n",
    "# batch_size_options = [8, 16, 32] \n",
    "\n",
    "# all_run_results = [] \n",
    "# results_csv_path = './roberta_grid_search_results_simplified.csv' \n",
    "\n",
    "\n",
    "# if os.path.exists(results_csv_path):\n",
    "#     print(f\"Appending to existing results file: {results_csv_path}\")\n",
    "\n",
    "# for current_batch_size in batch_size_options:\n",
    "#     for current_epochs in epochs_options:\n",
    "#         for current_lr in learning_rates:\n",
    "#             print(f\"\\n--- Training: BS={current_batch_size}, Epochs={current_epochs}, LR={current_lr} ---\")\n",
    "            \n",
    "#             K.clear_session() \n",
    "#             gc.collect()\n",
    "\n",
    "#             train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#                 dict(train_encodings), y_train_int\n",
    "#             )).shuffle(buffer_size=len(texts_train), seed=GLOBAL_SEED).batch(current_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            \n",
    "#             val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#                 dict(val_encodings), y_val_int\n",
    "#             )).batch(current_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#             model = TFRobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "#             optimizer = tf.keras.optimizers.AdamW(learning_rate=current_lr)\n",
    "#             loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#             metrics_list = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')] \n",
    "#             model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics_list)\n",
    "            \n",
    "#             history = model.fit(\n",
    "#                 train_dataset,\n",
    "#                 validation_data=val_dataset,\n",
    "#                 epochs=current_epochs,\n",
    "#                 verbose=1 \n",
    "#             )\n",
    "            \n",
    "#             final_train_accuracy = history.history['accuracy'][-1] if 'accuracy' in history.history and history.history['accuracy'] else None\n",
    "#             final_val_accuracy = history.history['val_accuracy'][-1] if 'val_accuracy' in history.history and history.history['val_accuracy'] else None\n",
    "            \n",
    "#             run_result = {\n",
    "#                 'batch_size': current_batch_size, \n",
    "#                 'epochs': current_epochs, \n",
    "#                 'learning_rate': current_lr,\n",
    "#                 'train_accuracy': final_train_accuracy,\n",
    "#                 'val_accuracy': final_val_accuracy,\n",
    "#             }\n",
    "#             all_run_results.append(run_result)\n",
    "            \n",
    "#             df_current_run = pd.DataFrame([run_result])\n",
    "#             if not os.path.exists(results_csv_path) or os.path.getsize(results_csv_path) == 0:\n",
    "#                 df_current_run.to_csv(results_csv_path, index=False, header=True)\n",
    "#             else:\n",
    "#                 df_current_run.to_csv(results_csv_path, index=False, header=False, mode='a')\n",
    "#             print(f\"  Results appended to {results_csv_path}\")\n",
    "\n",
    "#             del model, optimizer, history \n",
    "#             del train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4166d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_LR = 5e-5\n",
    "BEST_EPOCHS = 3\n",
    "BEST_BATCH_SIZE = 16\n",
    "\n",
    "train_dataset_amazon = tf.data.Dataset.from_tensor_slices((dict(train_encodings_amazon), y_train_amazon_int))\n",
    "\n",
    "train_dataset_amazon = train_dataset_amazon.shuffle(len(texts_train_amazon), seed=GLOBAL_SEED) \\\n",
    "                                           .batch(BEST_BATCH_SIZE) \\\n",
    "                                           .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "K.clear_session() \n",
    "gc.collect()\n",
    "\n",
    "model = TFRobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=BEST_LR) \n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset_amazon,\n",
    "          epochs=BEST_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab86637",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined_test_list = X_amazon_test_raw.tolist()\n",
    "y_combined_test_int_list = y_amazon_test_int.tolist()\n",
    "\n",
    "X_combined_test_list.extend(X_yelp_test_raw.tolist())\n",
    "y_combined_test_int_list.extend(y_yelp_test_int.tolist())\n",
    "\n",
    "y_combined_test_int = np.array(y_combined_test_int_list)\n",
    "\n",
    "combined_test_encodings = tokenizer(X_combined_test_list, truncation=True, padding=True, max_length=max_length, return_tensors=\"tf\")\n",
    "\n",
    "combined_test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(combined_test_encodings),\n",
    "    y_combined_test_int\n",
    ")).batch(BEST_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "combined_test_predictions_logits = model.predict(combined_test_dataset)\n",
    "predicted_logits_combined = combined_test_predictions_logits.logits \n",
    "y_pred_combined_int = np.argmax(predicted_logits_combined, axis=1)\n",
    "\n",
    "target_names_combined = ['CG', 'OR']\n",
    "\n",
    "accuracy_combined = accuracy_score(y_combined_test_int, y_pred_combined_int)\n",
    "precision_combined = precision_score(y_combined_test_int, y_pred_combined_int, average=None, labels=[0, 1], zero_division=0)\n",
    "recall_combined = recall_score(y_combined_test_int, y_pred_combined_int, average=None, labels=[0, 1], zero_division=0)\n",
    "f1_combined = f1_score(y_combined_test_int, y_pred_combined_int, average=None, labels=[0, 1], zero_division=0)\n",
    "\n",
    "print(f\"\\nTikslumas: {accuracy_combined:.4f}\")\n",
    "if len(precision_combined) == 2 : \n",
    "    print(f\"Preciziškumas ({target_names_combined[0]}): {precision_combined[0]:.4f}\")\n",
    "    print(f\"Preciziškumas ({target_names_combined[1]}):    {precision_combined[1]:.4f}\")\n",
    "    print(f\"Atkūrimas ({target_names_combined[0]}):    {recall_combined[0]:.4f}\")\n",
    "    print(f\"Atkūrimas ({target_names_combined[1]}):       {recall_combined[1]:.4f}\")\n",
    "    print(f\"F1-Rezultatas ({target_names_combined[0]}):  {f1_combined[0]:.4f}\")\n",
    "    print(f\"F1-Rezultatas ({target_names_combined[1]}):     {f1_combined[1]:.4f}\")\n",
    "else: \n",
    "    print(f\"Preciziškumas: {precision_combined}\")\n",
    "    print(f\"Atkūrimas: {recall_combined}\")\n",
    "    print(f\"F1-statistikos reikšmė: {f1_combined}\")\n",
    "\n",
    "cm_combined = confusion_matrix(y_combined_test_int, y_pred_combined_int, labels=[0, 1])\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_combined, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names_combined, yticklabels=target_names_combined)\n",
    "plt.title('Painiavos Matrica - Kombinuotas Testų Rinkinys')\n",
    "plt.xlabel('Nuspėtos žymės')\n",
    "plt.ylabel('Tikros žymės')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report_combined = classification_report(y_combined_test_int, y_pred_combined_int, target_names=target_names_combined, zero_division=0)\n",
    "print(report_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_path = '../Experiment/Datasets/Google Play App Store For Testing.csv' \n",
    "df_new_test = pd.read_csv(new_dataset_path)\n",
    "\n",
    "text_column_new = 'review' \n",
    "label_column_new = 'label'  \n",
    "\n",
    "X_new_test_text = df_new_test[text_column_new].tolist()\n",
    "y_new_test_original = df_new_test[label_column_new]\n",
    "\n",
    "new_label_map = {'fake': 0, 'genuine': 1} \n",
    "y_new_test_int = y_new_test_original.map(new_label_map).values\n",
    "\n",
    "new_test_encodings = tokenizer(X_new_test_text, truncation=True, padding=True, max_length=max_length, return_tensors=\"tf\")\n",
    "\n",
    "new_tf_test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(new_test_encodings),\n",
    "    y_new_test_int\n",
    ")).batch(BEST_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "new_test_predictions_logits = model.predict(new_tf_test_dataset)\n",
    "new_predicted_logits = new_test_predictions_logits.logits\n",
    "y_pred_new_test_int = np.argmax(new_predicted_logits, axis=1)\n",
    "\n",
    "target_names_new_test = ['fake', 'genuine'] \n",
    "\n",
    "accuracy_new_test = accuracy_score(y_new_test_int, y_pred_new_test_int)\n",
    "precision_new_test = precision_score(y_new_test_int, y_pred_new_test_int, average=None, labels=[0, 1], zero_division=0)\n",
    "recall_new_test = recall_score(y_new_test_int, y_pred_new_test_int, average=None, labels=[0, 1], zero_division=0)\n",
    "f1_new_test = f1_score(y_new_test_int, y_pred_new_test_int, average=None, labels=[0, 1], zero_division=0)\n",
    "\n",
    "print(f\"\\nTikslumas: {accuracy_new_test:.4f}\")\n",
    "print(f\"Preciziškumas ({target_names_new_test[0]}): {precision_new_test[0]:.4f}\")\n",
    "print(f\"Preciziškumas ({target_names_new_test[1]}):    {precision_new_test[1]:.4f}\")\n",
    "print(f\"Atkūrimas ({target_names_new_test[0]}):    {recall_new_test[0]:.4f}\")\n",
    "print(f\"Atkūrimas ({target_names_new_test[1]}):       {recall_new_test[1]:.4f}\")\n",
    "print(f\"F1-statistikos reikšmė ({target_names_new_test[0]}):  {f1_new_test[0]:.4f}\")\n",
    "print(f\"F1-statistikos reikšmė ({target_names_new_test[1]}):     {f1_new_test[1]:.4f}\")\n",
    "\n",
    "cm_new_test = confusion_matrix(y_new_test_int, y_pred_new_test_int, labels=[0, 1])\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_new_test, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names_new_test, yticklabels=target_names_new_test)\n",
    "plt.title('Painiavos Matrica')\n",
    "plt.xlabel('Nuspėtos žymės')\n",
    "plt.ylabel('Tikros žymės')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report \n",
    "report_new_test = classification_report(y_new_test_int, y_pred_new_test_int, target_names=target_names_new_test, zero_division=0)\n",
    "print(report_new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import numpy as np\n",
    "\n",
    "batch_sizes    = np.array([8]*9 + [16]*9 + [32]*9)\n",
    "epochs         = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3] * 3)\n",
    "learning_rates = np.array([1e-05, 3e-05, 5e-05] * 9)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax  = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(batch_sizes, epochs, learning_rates, marker='o', s=50, color='orange', edgecolors='k')\n",
    "\n",
    "ax.set_xlabel('Partijos dydis', labelpad=10)\n",
    "ax.set_ylabel('Epochos',      labelpad=10)\n",
    "\n",
    "ax.set_xticks([8, 16, 32])\n",
    "ax.set_yticks([1, 2, 3])\n",
    "ax.set_zticks([1e-05, 3e-05, 5e-05])\n",
    "ax.set_zticklabels(['1e-05', '3e-05', '5e-05'])\n",
    "ax.tick_params(axis='z', pad=8)  \n",
    "\n",
    "plt.tight_layout(rect=(0, 0, 0.88, 1))\n",
    "\n",
    "fig.text(\n",
    "    0.88, 0.5, 'Mokymosi sparta',\n",
    "    va='center', ha='center', rotation='vertical', fontsize=10  \n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf216",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
