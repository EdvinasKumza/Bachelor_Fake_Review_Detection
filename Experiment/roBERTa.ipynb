{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8218ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\" \n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification\n",
    "\n",
    "info = tf.sysconfig.get_build_info()\n",
    "print(\"Built against CUDA:\",  info.get(\"cuda_version\"))\n",
    "print(\"Built against cuDNN:\", info.get(\"cudnn_version\"))\n",
    "print(\"GPUs found: \",         tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90876108",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SEED = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(GLOBAL_SEED)\n",
    "tf.random.set_seed(GLOBAL_SEED)\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"TF:\",     tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../Experiment/Datasets/Generated Fake Amazon Reviews Dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Amazon dataset shape:\", df.shape)\n",
    "print(df.head())\n",
    "print(df.columns.tolist())\n",
    "\n",
    "df_amazon_phase1_data, df_amazon_test_data = train_test_split(\n",
    "    df, test_size=0.2, random_state=GLOBAL_SEED, stratify=df['label']\n",
    ")\n",
    "\n",
    "X_amazon_test_raw = df_amazon_test_data['text_']\n",
    "y_amazon_test_raw = df_amazon_test_data['label']\n",
    "\n",
    "X_train_amazon_text = df_amazon_phase1_data['text_']\n",
    "y_train_amazon_raw = df_amazon_phase1_data['label']\n",
    "\n",
    "texts_train_amazon = X_train_amazon_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda1ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_file_path = '../Experiment/Datasets/Mixed Yelp Dataset.csv'\n",
    "df_yelp = pd.read_csv(yelp_file_path)\n",
    "\n",
    "print(\"Yelp dataset shape:\", df_yelp.shape)\n",
    "print(df_yelp.head())\n",
    "print(df_yelp.columns.tolist())\n",
    "\n",
    "df_yelp_phase2_data, df_yelp_test_data = train_test_split(\n",
    "    df_yelp, test_size=0.2, random_state=GLOBAL_SEED, stratify=df_yelp['LABEL']\n",
    ")\n",
    "\n",
    "X_yelp_test_raw = df_yelp_test_data['REVIEW_TEXT']\n",
    "y_yelp_test_raw = df_yelp_test_data['LABEL'].replace({-1: 'CG', 1: 'OR'})\n",
    "    \n",
    "X_train_yelp_text = df_yelp_phase2_data['REVIEW_TEXT']\n",
    "y_train_yelp_raw = df_yelp_phase2_data['LABEL'].replace({-1: 'CG', 1: 'OR'}) \n",
    "        \n",
    "texts_train_yelp = X_train_yelp_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d4fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'FacebookAI/roberta-base' \n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "num_labels = 2 \n",
    "\n",
    "label_map = {'CG': 0, 'OR': 1} \n",
    "\n",
    "y_train_amazon_int = y_train_amazon_raw.map(label_map).values \n",
    "y_amazon_test_int = y_amazon_test_raw.map(label_map).values\n",
    "\n",
    "y_train_yelp_int = y_train_yelp_raw.map(label_map).values\n",
    "y_yelp_test_int = y_yelp_test_raw.map(label_map).values\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "train_encodings_amazon = tokenizer(texts_train_amazon, truncation=True, padding=True, max_length=max_length, return_tensors=\"tf\")\n",
    "train_encodings_yelp = tokenizer(texts_train_yelp, truncation=True, padding=True, max_length=max_length, return_tensors=\"tf\")\n",
    "\n",
    "texts_train_mixed = texts_train_amazon + texts_train_yelp\n",
    "y_train_mixed_int = np.concatenate([y_train_amazon_int, y_train_yelp_int])\n",
    "train_encodings_mixed = tokenizer(texts_train_mixed, truncation=True, padding=True, max_length=max_length, return_tensors=\"tf\")\n",
    "\n",
    "\n",
    "MIXED_LR = 5e-5\n",
    "MIXED_EPOCHS = 1\n",
    "MIXED_BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "AMAZON_LR = 5e-5\n",
    "AMAZON_EPOCHS = 1\n",
    "AMAZON_BATCH_SIZE = 16\n",
    "\n",
    "YELP_LR = 1e-5\n",
    "YELP_EPOCHS = 1\n",
    "YELP_BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762746a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametrų tinklelio paieška\n",
    "\n",
    "# learning_rates = [1e-5, 3e-5, 5e-5]\n",
    "# epochs_options = [1, 2, 3]\n",
    "# batch_size_options = [8, 16, 32] \n",
    "\n",
    "# all_run_results = [] \n",
    "# results_csv_path = './roberta_grid_search_results_simplified.csv' \n",
    "\n",
    "\n",
    "# if os.path.exists(results_csv_path):\n",
    "#     print(f\"Appending to existing results file: {results_csv_path}\")\n",
    "\n",
    "# for current_batch_size in batch_size_options:\n",
    "#     for current_epochs in epochs_options:\n",
    "#         for current_lr in learning_rates:\n",
    "#             print(f\"\\n--- Training: BS={current_batch_size}, Epochs={current_epochs}, LR={current_lr} ---\")\n",
    "            \n",
    "#             K.clear_session() \n",
    "#             gc.collect()\n",
    "\n",
    "#             train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#                 dict(train_encodings), y_train_int\n",
    "#             )).shuffle(buffer_size=len(texts_train), seed=GLOBAL_SEED).batch(current_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            \n",
    "#             val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#                 dict(val_encodings), y_val_int\n",
    "#             )).batch(current_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#             model = TFRobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "#             optimizer = tf.keras.optimizers.AdamW(learning_rate=current_lr)\n",
    "#             loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#             metrics_list = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')] \n",
    "#             model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics_list)\n",
    "            \n",
    "#             history = model.fit(\n",
    "#                 train_dataset,\n",
    "#                 validation_data=val_dataset,\n",
    "#                 epochs=current_epochs,\n",
    "#                 verbose=1 \n",
    "#             )\n",
    "            \n",
    "#             final_train_accuracy = history.history['accuracy'][-1] if 'accuracy' in history.history and history.history['accuracy'] else None\n",
    "#             final_val_accuracy = history.history['val_accuracy'][-1] if 'val_accuracy' in history.history and history.history['val_accuracy'] else None\n",
    "            \n",
    "#             run_result = {\n",
    "#                 'batch_size': current_batch_size, \n",
    "#                 'epochs': current_epochs, \n",
    "#                 'learning_rate': current_lr,\n",
    "#                 'train_accuracy': final_train_accuracy,\n",
    "#                 'val_accuracy': final_val_accuracy,\n",
    "#             }\n",
    "#             all_run_results.append(run_result)\n",
    "            \n",
    "#             df_current_run = pd.DataFrame([run_result])\n",
    "#             if not os.path.exists(results_csv_path) or os.path.getsize(results_csv_path) == 0:\n",
    "#                 df_current_run.to_csv(results_csv_path, index=False, header=True)\n",
    "#             else:\n",
    "#                 df_current_run.to_csv(results_csv_path, index=False, header=False, mode='a')\n",
    "#             print(f\"  Results appended to {results_csv_path}\")\n",
    "\n",
    "#             del model, optimizer, history \n",
    "#             del train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068adb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_mixed = tf.data.Dataset.from_tensor_slices((dict(train_encodings_mixed), y_train_mixed_int))\n",
    "train_dataset_mixed = train_dataset_mixed.shuffle(len(texts_train_mixed), seed=GLOBAL_SEED) \\\n",
    "                                         .batch(MIXED_BATCH_SIZE) \\\n",
    "                                         .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "K.clear_session() \n",
    "gc.collect()\n",
    "\n",
    "model_mixed = TFRobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=MIXED_LR) \n",
    "model_mixed.compile(optimizer=optimizer,\n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history_mixed = model_mixed.fit(train_dataset_mixed, epochs=MIXED_EPOCHS)\n",
    "\n",
    "# Test\n",
    "X_combined_test_list = X_amazon_test_raw.tolist()\n",
    "y_combined_test_int_list = y_amazon_test_int.tolist()\n",
    "\n",
    "X_combined_test_list.extend(X_yelp_test_raw.tolist())\n",
    "y_combined_test_int_list.extend(y_yelp_test_int.tolist())\n",
    "\n",
    "y_combined_test_int = np.array(y_combined_test_int_list)\n",
    "X_combined_test_raw = pd.Series(X_combined_test_list)\n",
    "\n",
    "combined_test_encodings = tokenizer(X_combined_test_raw.tolist(), truncation=True, padding=True, \n",
    "                                   max_length=max_length, return_tensors=\"tf\")\n",
    "\n",
    "combined_test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(combined_test_encodings),\n",
    "    y_combined_test_int\n",
    ")).batch(MIXED_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "combined_test_predictions = model_mixed.predict(combined_test_dataset)\n",
    "combined_predicted_logits = combined_test_predictions.logits\n",
    "combined_y_pred_int = np.argmax(combined_predicted_logits, axis=1)\n",
    "\n",
    "target_names_combined = ['CG', 'OR']\n",
    "\n",
    "accuracy_combined = accuracy_score(y_combined_test_int, combined_y_pred_int)\n",
    "precision_combined = precision_score(y_combined_test_int, combined_y_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "recall_combined = recall_score(y_combined_test_int, combined_y_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "f1_combined = f1_score(y_combined_test_int, combined_y_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "\n",
    "print(f\"Tikslumas: {accuracy_combined:.4f}\")\n",
    "print(f\"Preciziškumas (Netikras/CG): {precision_combined[0]:.4f}, Preciziškumas (Tikras/OR): {precision_combined[1]:.4f}\")\n",
    "print(f\"Atkūrimas (Netikras/CG): {recall_combined[0]:.4f}, Atkūrimas (Tikras/OR): {recall_combined[1]:.4f}\")\n",
    "print(f\"F1-Statistikos reikšmė (Netikras/CG): {f1_combined[0]:.4f}, F1-Statistikos reikšmė (Tikras/OR): {f1_combined[1]:.4f}\")\n",
    "\n",
    "print(\"\\nSujungto duomenų rinkinio modelio rezultatų lentelė:\")\n",
    "result_table = pd.DataFrame({\n",
    "    'Klasė': ['Netikras (CG)', 'Tikras (OR)'],\n",
    "    'Preciziškumas': [precision_combined[0], precision_combined[1]],\n",
    "    'Atkūrimas': [recall_combined[0], recall_combined[1]],\n",
    "    'F1-Statistikos reikšmė': [f1_combined[0], f1_combined[1]],\n",
    "})\n",
    "result_table['Tikslumas'] = accuracy_combined\n",
    "print(result_table.to_string(index=False))\n",
    "\n",
    "cm_combined = confusion_matrix(y_combined_test_int, combined_y_pred_int, labels=[0, 1])\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_combined, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names_combined, yticklabels=target_names_combined)\n",
    "plt.title('Painiavos Matrica - Modelis ant sujungto testinio duomenų rinkinio')\n",
    "plt.xlabel('Prognozuojamos Etiketės')\n",
    "plt.ylabel('Tikrosios Etiketės')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ccad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_amazon = tf.data.Dataset.from_tensor_slices((dict(train_encodings_amazon), y_train_amazon_int))\n",
    "train_dataset_amazon = train_dataset_amazon.shuffle(len(texts_train_amazon), seed=GLOBAL_SEED) \\\n",
    "                                          .batch(AMAZON_BATCH_SIZE) \\\n",
    "                                          .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "K.clear_session() \n",
    "gc.collect()\n",
    "\n",
    "model_amazon = TFRobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=AMAZON_LR) \n",
    "model_amazon.compile(optimizer=optimizer,\n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history_amazon = model_amazon.fit(train_dataset_amazon, epochs=AMAZON_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ed1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_test_encodings = tokenizer(X_amazon_test_raw.tolist(), truncation=True, padding=True, \n",
    "                                 max_length=max_length, return_tensors=\"tf\")\n",
    "\n",
    "amazon_test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(amazon_test_encodings),\n",
    "    y_amazon_test_int\n",
    ")).batch(AMAZON_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "amazon_test_predictions = model_amazon.predict(amazon_test_dataset)\n",
    "amazon_predicted_logits = amazon_test_predictions.logits\n",
    "amazon_y_pred_int = np.argmax(amazon_predicted_logits, axis=1)\n",
    "\n",
    "accuracy_amazon = accuracy_score(y_amazon_test_int, amazon_y_pred_int)\n",
    "precision_amazon = precision_score(y_amazon_test_int, amazon_y_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "recall_amazon = recall_score(y_amazon_test_int, amazon_y_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "f1_amazon = f1_score(y_amazon_test_int, amazon_y_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "\n",
    "print(f\"Tikslumas: {accuracy_amazon:.4f}\")\n",
    "print(f\"Preciziškumas (Netikras/CG): {precision_amazon[0]:.4f}, Preciziškumas (Tikras/OR): {precision_amazon[1]:.4f}\")\n",
    "print(f\"Atkūrimas (Netikras/CG): {recall_amazon[0]:.4f}, Atkūrimas (Tikras/OR): {recall_amazon[1]:.4f}\")\n",
    "print(f\"F1-Statistikos reikšmė (Netikras/CG): {f1_amazon[0]:.4f}, F1-Statistikos reikšmė (Tikras/OR): {f1_amazon[1]:.4f}\")\n",
    "\n",
    "print(\"\\nAmazon Modelio Rezultatų Lentelė:\")\n",
    "result_table_amazon = pd.DataFrame({\n",
    "    'Klasė': ['CG', 'OR'],\n",
    "    'Preciziškumas': [precision_amazon[0], precision_amazon[1]],\n",
    "    'Atkūrimas': [recall_amazon[0], recall_amazon[1]],\n",
    "    'F1-Statistikos reikšmė': [f1_amazon[0], f1_amazon[1]],\n",
    "})\n",
    "result_table_amazon['Tikslumas'] = accuracy_amazon\n",
    "print(result_table_amazon.to_string(index=False))\n",
    "\n",
    "cm_amazon = confusion_matrix(y_amazon_test_int, amazon_y_pred_int, labels=[0, 1])\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_amazon, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names_combined, yticklabels=target_names_combined)\n",
    "plt.title('Painiavos Matrica - Amazon Modelis ant amazon testinio duomenų rinkinio')\n",
    "plt.xlabel('Prognozuojamos Etiketės')\n",
    "plt.ylabel('Tikrosios Etiketės')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd09c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_yelp = tf.data.Dataset.from_tensor_slices((dict(train_encodings_yelp), y_train_yelp_int))\n",
    "train_dataset_yelp = train_dataset_yelp.shuffle(len(texts_train_yelp), seed=GLOBAL_SEED) \\\n",
    "                                      .batch(YELP_BATCH_SIZE) \\\n",
    "                                      .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "K.clear_session() \n",
    "gc.collect()\n",
    "\n",
    "model_yelp = TFRobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=YELP_LR) \n",
    "model_yelp.compile(optimizer=optimizer,\n",
    "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "history_yelp = model_yelp.fit(train_dataset_yelp, epochs=YELP_EPOCHS)\n",
    "\n",
    "yelp_test_encodings = tokenizer(X_yelp_test_raw.tolist(), truncation=True, padding=True, \n",
    "                               max_length=max_length, return_tensors=\"tf\")\n",
    "\n",
    "yelp_test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(yelp_test_encodings),\n",
    "    y_yelp_test_int\n",
    ")).batch(YELP_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "yelp_test_predictions = model_yelp.predict(yelp_test_dataset)\n",
    "yelp_predicted_logits = yelp_test_predictions.logits\n",
    "yelp_y_pred_int = np.argmax(yelp_predicted_logits, axis=1)\n",
    "\n",
    "accuracy_yelp = accuracy_score(y_yelp_test_int, yelp_y_pred_int)\n",
    "precision_yelp = precision_score(y_yelp_test_int, yelp_y_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "recall_yelp = recall_score(y_yelp_test_int, yelp_y_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "f1_yelp = f1_score(y_yelp_test_int, yelp_y_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "\n",
    "print(f\"Tikslumas: {accuracy_yelp:.4f}\")\n",
    "print(f\"Preciziškumas (Netikras/CG): {precision_yelp[0]:.4f}, Preciziškumas (Tikras/OR): {precision_yelp[1]:.4f}\")\n",
    "print(f\"Atkūrimas (Netikras/CG): {recall_yelp[0]:.4f}, Atkūrimas (Tikras/OR): {recall_yelp[1]:.4f}\")\n",
    "print(f\"F1-Statistikos reikšmė (Netikras/CG): {f1_yelp[0]:.4f}, F1-Statistikos reikšmė (Tikras/OR): {f1_yelp[1]:.4f}\")\n",
    "\n",
    "result_table_yelp = pd.DataFrame({\n",
    "    'Klasė': ['-1 (Netikras)', '1 (Tikras)'],\n",
    "    'Preciziškumas': [precision_yelp[0], precision_yelp[1]],\n",
    "    'Atkūrimas': [recall_yelp[0], recall_yelp[1]],\n",
    "    'F1-Statistikos reikšmė': [f1_yelp[0], f1_yelp[1]],\n",
    "})\n",
    "result_table_yelp['Tikslumas'] = accuracy_yelp\n",
    "print(result_table_yelp.to_string(index=False))\n",
    "\n",
    "cm_yelp = confusion_matrix(y_yelp_test_int, yelp_y_pred_int, labels=[0, 1])\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_yelp, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names_combined, yticklabels=target_names_combined)\n",
    "plt.title('Painiavos Matrica - Yelp Modelis ant yelp testinio duomenų rinkinio')\n",
    "plt.xlabel('Prognozuojamos Etiketės')\n",
    "plt.ylabel('Tikrosios Etiketės')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67558df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_path = '../Experiment/Datasets/Google Play App Store For Testing.csv' \n",
    "df_new_test = pd.read_csv(new_dataset_path)\n",
    "\n",
    "print(df_new_test.shape)\n",
    "print(df_new_test.head())\n",
    "\n",
    "text_column_new = 'review' \n",
    "label_column_new = 'label'  \n",
    "\n",
    "X_new_test_text = df_new_test[text_column_new]\n",
    "y_new_test_original = df_new_test[label_column_new]\n",
    "\n",
    "new_label_map = {'fake': 0, 'genuine': 1} \n",
    "y_new_test_int = y_new_test_original.map(new_label_map).values\n",
    "\n",
    "target_names_new_test = ['fake', 'genuine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gplay_test_encodings = tokenizer(X_new_test_text.tolist(), truncation=True, padding=True, \n",
    "                                max_length=max_length, return_tensors=\"tf\")\n",
    "\n",
    "gplay_test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(gplay_test_encodings),\n",
    "    y_new_test_int\n",
    ")).batch(AMAZON_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "amazon_gplay_predictions = model_amazon.predict(gplay_test_dataset)\n",
    "amazon_gplay_logits = amazon_gplay_predictions.logits\n",
    "amazon_gplay_pred_int = np.argmax(amazon_gplay_logits, axis=1)\n",
    "\n",
    "accuracy_amazon_gplay = accuracy_score(y_new_test_int, amazon_gplay_pred_int)\n",
    "precision_amazon_gplay = precision_score(y_new_test_int, amazon_gplay_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "recall_amazon_gplay = recall_score(y_new_test_int, amazon_gplay_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "f1_amazon_gplay = f1_score(y_new_test_int, amazon_gplay_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "\n",
    "print(f\"Tikslumas: {accuracy_amazon_gplay:.4f}\")\n",
    "print(f\"Preciziškumas (Netikras): {precision_amazon_gplay[0]:.4f}, Preciziškumas (Tikras): {precision_amazon_gplay[1]:.4f}\")\n",
    "print(f\"Atkūrimas (Netikras): {recall_amazon_gplay[0]:.4f}, Atkūrimas (Tikras): {recall_amazon_gplay[1]:.4f}\")\n",
    "print(f\"F1-Statistikos reikšmė (Netikras): {f1_amazon_gplay[0]:.4f}, F1-Statistikos reikšmė (Tikras): {f1_amazon_gplay[1]:.4f}\")\n",
    "\n",
    "cm_amazon_gplay = confusion_matrix(y_new_test_int, amazon_gplay_pred_int, labels=[0, 1])\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_amazon_gplay, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names_new_test, yticklabels=target_names_new_test)\n",
    "plt.title('Painiavos Matrica - Amazon duomenų apmokytas modelis ant Google Play duomenų rinkinio')\n",
    "plt.xlabel('Prognozuojamos Etiketės')\n",
    "plt.ylabel('Tikrosios Etiketės')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a50085",
   "metadata": {},
   "outputs": [],
   "source": [
    "gplay_yelp_test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(gplay_test_encodings),\n",
    "    y_new_test_int\n",
    ")).batch(YELP_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "yelp_gplay_predictions = model_yelp.predict(gplay_yelp_test_dataset)\n",
    "yelp_gplay_logits = yelp_gplay_predictions.logits\n",
    "yelp_gplay_pred_int = np.argmax(yelp_gplay_logits, axis=1)\n",
    "\n",
    "accuracy_yelp_gplay = accuracy_score(y_new_test_int, yelp_gplay_pred_int)\n",
    "precision_yelp_gplay = precision_score(y_new_test_int, yelp_gplay_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "recall_yelp_gplay = recall_score(y_new_test_int, yelp_gplay_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "f1_yelp_gplay = f1_score(y_new_test_int, yelp_gplay_pred_int, average=None, labels=[0, 1], zero_division=0)\n",
    "\n",
    "print(f\"Tikslumas: {accuracy_yelp_gplay:.4f}\")\n",
    "print(f\"Preciziškumas (Netikras): {precision_yelp_gplay[0]:.4f}, Preciziškumas (Tikras): {precision_yelp_gplay[1]:.4f}\")\n",
    "print(f\"Atkūrimas (Netikras): {recall_yelp_gplay[0]:.4f}, Atkūrimas (Tikras): {recall_yelp_gplay[1]:.4f}\")\n",
    "print(f\"F1-Statistikos reikšmė (Netikras): {f1_yelp_gplay[0]:.4f}, F1-Statistikos reikšmė (Tikras): {f1_yelp_gplay[1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf216",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
